<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="or-3-chapter-14---stochastic-games"><span class="header-section-number">1</span> OR 3: Chapter 14 - Stochastic games</h1>
<h2 id="recap"><span class="header-section-number">1.1</span> Recap</h2>
<p>In the <a href="http://drvinceknight.github.io/Year_3_game_theory_course/Course_Notes/Chapter_13_Random_events_and_incomplete_information.html">previous chapter</a>:</p>
<ul>
<li>We considered games of incomplete information;</li>
<li>Discussed some basic utility theory;</li>
<li>Considered the principal agent game.</li>
</ul>
<p>In this chapter we will take a look at a more general type of random game.</p>
<h2 id="stochastic-games"><span class="header-section-number">1.2</span> Stochastic games</h2>
<h3 id="definition-of-a-stochastic-game"><span class="header-section-number">1.2.1</span> Definition of a stochastic game</h3>
<hr />
<p>A stochastic game is defined by:</p>
<ul>
<li>X a set of states with a stage game defined for each state;</li>
<li>A set of strategies <span class="math">\(S_i(x)\)</span> for each player for each state <span class="math">\(x\in X\)</span>;</li>
<li>A set of rewards dependant on the state and the actions of the other players: <span class="math">\(u_i(x,s_1,s_2)\)</span>;</li>
<li>A set of probabilities of transitioning to a future state: <span class="math">\(\pi(x&#39;|x,s_1,s_2)\)</span>;</li>
<li>Each stage game is played at a set of discrete times <span class="math">\(t\)</span>.</li>
</ul>
<hr />
<p>We will make some simplifying assumptions in this course:</p>
<ol style="list-style-type: decimal">
<li>The length of the game is not known (infinite horizon);</li>
<li>The rewards and transition probabilities are not dependent;</li>
<li>We will only consider strategies called <strong>Markov strategies</strong>.</li>
</ol>
<h3 id="definition-of-a-markov-strategy"><span class="header-section-number">1.2.2</span> Definition of a Markov strategy</h3>
<hr />
<p>A strategy is call a <strong>Markov strategy</strong> if the behaviour dictated is not time dependent.</p>
<hr />
<h3 id="example"><span class="header-section-number">1.2.3</span> Example</h3>
<p>Consider the following game with <span class="math">\(X=\{x,y\}\)</span>:</p>
<ul>
<li><span class="math">\(S_1(x)=\{a,b\}\)</span> and <span class="math">\(S_2(x)=\{c,d\}\)</span>;</li>
<li><span class="math">\(S_1(y)=\{e\}\)</span> and <span class="math">\(S_2(x)=\{f\}\)</span>;</li>
</ul>
<p>We have the stage game corresponding to state <span class="math">\(x\)</span>:</p>
<p><span class="math">\[
\begin{pmatrix}
(8,4)&amp;(5,3)\\
(1,5)&amp;(2,6)
\end{pmatrix}
\]</span></p>
<p>The stage game corresponding to state <span class="math">\(y\)</span>:</p>
<p><span class="math">\[
\begin{pmatrix}
(0,0)\\
\end{pmatrix}
\]</span></p>
<p>The transition probabilities corresponding to state <span class="math">\(x\)</span>:</p>
<p><span class="math">\[
\begin{pmatrix}
(.5,.5)&amp;(1,0)\\
(1,0)&amp;(1,0)
\end{pmatrix}
\]</span></p>
<p>The transition probabilities corresponding to state <span class="math">\(y\)</span>:</p>
<p><span class="math">\[
\begin{pmatrix}
(0,0)\\
\end{pmatrix}
\]</span></p>
<p>A concise way of representing all this is shown.</p>
<div class="figure">
<img src="http://drvinceknight.github.io/Year_3_game_theory_course/Course_Notes/images/L14-img01.png" />
</div>
<p>We see that the Nash equilibrium for the stage game corresponding to <span class="math">\(x\)</span> is <span class="math">\((a,c)\)</span> however as soon as the players play that strategy profile they will go to state <span class="math">\(y\)</span> which is an absorbing state at which players gain no further utility.</p>
<p>To calculate utilities for players in infinite horizon stochastic games we use a discount rate. Thus without loss of generality if the game is in state <span class="math">\(x\)</span> and we assume that both players are playing <span class="math">\(\sigma^*_i\)</span> then player 1 would be attempting to maximise future payoffs:</p>
<p><span class="math">\[U_1(r,s)=\left(u_1(x,r,s)+\delta\sum_{x&#39;\in X}\pi(x&#39;|x,r,s)U_1^*(x&#39;)\right)\]</span></p>
<p>where <span class="math">\(U_1^*\)</span> denotes the expected utility to player 1 when both players are playing the Nash strategy profile.</p>
<p>Thus a Nash equilibrium satisfies:</p>
<p><span class="math">\[U_1^*(x)=\max_{r\in S_1(x)}(u_i(x,r,s^*)+\delta\sum_{x&#39;\in X}\pi(x&#39;|x,r,s^*)U_1^*(x&#39;)\]</span> <span class="math">\[U_2^*(x)=\max_{s\in S_2(x)}(u_i(x,r^*,s)+\delta\sum_{x&#39;\in X}\pi(x&#39;|x,r^*,s)U_1^*(x&#39;)\]</span></p>
<p>Solving these equations is not straightforward. We will take a look at one approach by solving the example we have above.</p>
<h2 id="finding-equilibria-in-stochastic-games"><span class="header-section-number">1.3</span> Finding equilibria in stochastic games</h2>
<p>Let us find a Nash equilibrium for the game considered above with <span class="math">\(\delta=2/3\)</span>.</p>
<p>State <span class="math">\(y\)</span> gives no value to either player so we only need to consider state <span class="math">\(x\)</span>. Let the future gains to player 1 in state <span class="math">\(x\)</span> be <span class="math">\(u\)</span>, and the future gains to player 2 in state <span class="math">\(x\)</span> be <span class="math">\(v\)</span>. Thus the players are facing the following game:</p>
<p><span class="math">\[\begin{pmatrix}
(8+\frac{1}{3}v,4+\frac{1}{3}u)&amp;(5+\frac{2}{3}v,3+\frac{2}{3}u)\\
(1+\frac{2}{3}v,5+\frac{2}{3}u)&amp;(2+\frac{2}{3}v,6+\frac{2}{3}u)
\end{pmatrix}\]</span></p>
<p>We consider each strategy pair and state the condition for Nash equilibrium:</p>
<ol style="list-style-type: decimal">
<li><span class="math">\((a,c)\)</span>: <span class="math">\(v\leq 21\)</span> and <span class="math">\(u\leq 3\)</span>.</li>
<li><span class="math">\((a,d)\)</span>: <span class="math">\(u\geq3\)</span>.</li>
<li><span class="math">\((b,c)\)</span>: <span class="math">\(v\geq 21\)</span> and <span class="math">\(5\geq 6\)</span>.</li>
<li><span class="math">\((b,d)\)</span>: <span class="math">\(5\geq2\)</span>.</li>
</ol>
<p>Now consider the implications of each of those profiles being an equilibrium:</p>
<ol style="list-style-type: decimal">
<li><span class="math">\(8+v/3=v\)</span> <span class="math">\(\Rightarrow\)</span> <span class="math">\(v=12\)</span> and <span class="math">\(4+u/3=u\)</span> <span class="math">\(\Rightarrow\)</span> <span class="math">\(u=6\)</span> which contradicts the corresponding inequality.</li>
<li><span class="math">\(3+2u/3=u\)</span> <span class="math">\(\Rightarrow\)</span> <span class="math">\(u=9\)</span>.</li>
<li>The inequality for <span class="math">\(u\)</span> cannot hold.</li>
<li>The inequality cannot hold.</li>
</ol>
<p>Thus the unique Markov strategy Nash equilibria is <span class="math">\((a,d)\)</span> <strong>which is not the stage Nash equilibria!</strong></p>
<p>(Other versions of the above: <a href="http://drvinceknight.github.io/Year_3_game_theory_course/Course_Notes/Chapter_14_Stochastic_games.pdf">pdf</a> <a href="http://drvinceknight.github.io/Year_3_game_theory_course/Course_Notes/Chapter_14_Stochastic_games.docx">docx (not recommended)</a>)</p>
</body>
</html>
